# Movies_ETL
### Overview:
I helped my friend Britta prepare for the Amazing Prime Hackathon. Britta asked me to gather data from Wikipedia and Kaggle, combine them, and save them into a SQL database so that the hackathon participants have a nice, clean dataset to use. To do so, I followed the ETL process: extract the Wikipedia and Kaggle data from their respective files, transform the datasets by cleaning them up and joining them together, and load the cleaned dataset into a SQL database. After completing the first database, Britta needed my help to create an automated pipeline that takes in new data, performs the appropriate transformations, and loads the data into existing tables. So, I refactored my original code and created one function that takes in the three files (you can find these data files in my resource folder). Finally, I performed the ETL process by adding the data to a PostgreSQL database. 
